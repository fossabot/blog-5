<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Docker on Moos3</title>
    <link>http://blog.guthnur.net/tags/docker/</link>
    <description>Recent content in Docker on Moos3</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>All rights reserved - 2015</copyright>
    <lastBuildDate>Tue, 22 Dec 2015 11:32:51 -0800</lastBuildDate>
    <atom:link href="http://blog.guthnur.net/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>kubernetes installation on ubuntu</title>
      <link>http://blog.guthnur.net/kubernetes-ubuntu-installation/</link>
      <pubDate>Tue, 22 Dec 2015 11:32:51 -0800</pubDate>
      
      <guid>http://blog.guthnur.net/kubernetes-ubuntu-installation/</guid>
      <description>

&lt;p&gt;In this article I will show you how to setup kubernetes on ubuntu 14.04 or newer. I recently had to do this for a project.
Below are the steps to complete this with a example pod.&lt;/p&gt;

&lt;h3 id=&#34;steps&#34;&gt;steps&lt;/h3&gt;

&lt;ol&gt;
&lt;li&gt;Become Root&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;sudo su -
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Lets get the pre-requisite software packages installed&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;apt-get update
apt-get install ssh
apt-get install docker.io
apt-get install curl
apt-get install git
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Password-less ssh login setup, accept all the default parameters in the prompt of the below command (required for Kubernetes installation)&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
e1:c9:a5:dd:80:ee:cd:f0:c8:11:6c:a5:d4:ba:ff:cc root@vkohli-Latitude-E7440
The key&#39;s randomart image is:
+--[ RSA 2048]----+
|          ...    |
|         + o.    |
|        o B.     |
|       + B..+    |
|        S o..    |
|       . *..     |
|        . *.     |
|         .  .o   |
|             .E  |
+-----------------+
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Copy the ssh id_rsa key locally&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ ssh-copy-id -i /root/.ssh/id_rsa.pub 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In case this fails you can do it by hand. By doing:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat /root/.ssh/id_rsa.pub &amp;gt;&amp;gt; /root/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Validate the password-less ssh-login&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ ssh root@127.0.0.1
root@vkohli-virtual-machine:~$ exit
logout
Connection to 127.0.0.1 closed
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Get the Kubernetes release bundle from the official github repository&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ wget https://github.com/GoogleCloudPlatform/kubernetes/releases/download/v1.0.1/kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;Untar the Kubernetes bundle in the same directory&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ tar -xvf kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;We will build the binaries of Kubernetes code specifically for ubuntu cluster&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ cd kubernetes/cluster/ubuntu
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Execute the following shell script&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ./build.sh
Download flannel release ...
Flannel version is 0.4.0
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                Dload  Upload   Total   Spent    Left  Speed
100   411    0   411    0     0    252      0 --:--:--  0:00:01 --:--:--   252
100 2393k  100 2393k    0     0   204k      0  0:00:11  0:00:11 --:--:--  388k
Download etcd release ...
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   410    0   410    0     0    272      0 --:--:--  0:00:01 --:--:--   272
100 3713k  100 3713k    0     0   286k      0  0:00:12  0:00:12 --:--:--  496k
Download kubernetes release ...
% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                Dload  Upload   Total   Spent    Left  Speed
100   396    0   396    0     0    279      0 --:--:--  0:00:01 --:--:--   279
100 97.8M  100 97.8M    0     0   715k      0  0:02:20  0:02:20 --:--:--  501k
~/kubernetes/cluster/ubuntu/kubernetes/server ~/kubernetes/cluster/ubuntu
~/kubernetes/cluster/ubuntu
Done! All your commands locate in ./binaries dir
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This shell script will download and build the latest version of K8s, etcd and flannel binaries which can be found at following location;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cd binaries
$ ls
kubectl  master  minion
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;kubectl binary controls the Kubernetes cluster manager and the folder master &amp;amp; minion contains the binaries built for the purpose of configuring K8s master and node respectively.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Configure the cluster information by editing only the following parameters of the file &lt;code&gt;cluster/ubuntu/config-default.sh&lt;/code&gt; in the editor of your choice.&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ cd
$ vi kubernetes/cluster/ubuntu/config-default.sh
export nodes=&amp;quot;root@127.0.0.1&amp;quot;
export roles=&amp;quot;ai&amp;quot;
export NUM_MINIONS=${NUM_MINIONS:-1}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Only update the above mentioned information in the file, rest of the configuration will remain as it is. The first variable nodes defines all the cluster nodes, in our case same machine will be configured as master and node so it contains only one entry.The role below “ai” specifies that same machine will act as master, “a” stands for master and “i” stands for node.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Now, we will be starting the cluster with the following command;&lt;/li&gt;
&lt;/ol&gt;

&lt;pre&gt;&lt;code&gt;$ cd kubernetes/cluster
$ KUBERNETES_PROVIDER=ubuntu ./kube-up.sh
Starting cluster using provider: ubuntu
... calling verify-prereqs
... calling kube-up
FLANNEL_NET
172.16.0.0/16
Deploying master and minion on machine 127.0.0.1

config-default.sh                                                                                100% 2904     2.8KB/s   00:00
util.sh                                                                                          100%   13KB  13.4KB/s   00:00
flanneld.conf                                                                                    100%  569     0.6KB/s   00:00
kube-controller-manager.conf                                                                     100%  746     0.7KB/s   00:00
kube-apiserver.conf                                                                              100%  676     0.7KB/s   00:00
etcd.conf                                                                                        100%  576     0.6KB/s   00:00
kube-scheduler.conf                                                                              100%  676     0.7KB/s   00:00
kube-apiserver                                                                                   100% 2358     2.3KB/s   00:00
kube-controller-manager                                                                          100% 2672     2.6KB/s   00:00
etcd                                                                                             100% 2073     2.0KB/s   00:00
flanneld                                                                                         100% 2159     2.1KB/s   00:00
kube-scheduler                                                                                   100% 2360     2.3KB/s   00:00
reconfDocker.sh                                                                                  100% 1493     1.5KB/s   00:00
kube-proxy.conf                                                                                  100%  648     0.6KB/s   00:00
flanneld.conf                                                                                    100%  569     0.6KB/s   00:00
kubelet.conf                                                                                     100%  634     0.6KB/s   00:00
etcd.conf                                                                                        100%  576     0.6KB/s   00:00
kube-proxy                                                                                       100% 2230     2.2KB/s   00:00
etcd                                                                                             100% 2073     2.0KB/s   00:00
flanneld                                                                                         100% 2159     2.1KB/s   00:00
kubelet                                                                                          100% 2162     2.1KB/s   00:00
kube-apiserver                                                                                   100%   34MB  33.7MB/s   00:00
kube-controller-manager                                                                          100%   26MB  26.2MB/s   00:00
etcdctl                                                                                          100% 6041KB   5.9MB/s   00:00
etcd                                                                                             100% 6494KB   6.3MB/s   00:00
flanneld                                                                                         100% 8695KB   8.5MB/s   00:00
kube-scheduler                                                                                   100%   17MB  17.0MB/s   00:00
kube-proxy                                                                                       100%   17MB  16.8MB/s   00:00
etcdctl                                                                                          100% 6041KB   5.9MB/s   00:00
etcd                                                                                             100% 6494KB   6.3MB/s   00:00
flanneld                                                                                         100% 8695KB   8.5MB/s   00:00
kubelet                                                                                          100%   33MB  33.2MB/s   00:01
[sudo] password to copy files and start node:
etcd start/running, process 1125
Connection to 127.0.0.1 closed.
Validating master
Validating root@127.0.0.1

Kubernetes cluster is running.  The master is running at:

  http://127.0.0.1

FLANNEL_NET
172.16.0.0/16
Using master 127.0.0.1
Wrote config for ubuntu to /home/root/.kube/config
... calling validate-cluster

Waiting for 1 ready nodes. 0 ready nodes, 0 registered. Retrying.
Found 1 nodes.
        NAME        LABELS                             STATUS
1       127.0.0.1   kubernetes.io/hostname=127.0.0.1   Ready
Validate output:
NAME                 STATUS    MESSAGE   ERROR
scheduler            Healthy   ok        nil
etcd-0               Healthy   {&amp;quot;action&amp;quot;:&amp;quot;get&amp;quot;,&amp;quot;node&amp;quot;:{&amp;quot;dir&amp;quot;:true,&amp;quot;nodes&amp;quot;:[{&amp;quot;key&amp;quot;:&amp;quot;/registry&amp;quot;,&amp;quot;dir&amp;quot;:true,&amp;quot;modifiedIndex&amp;quot;:3,&amp;quot;createdIndex&amp;quot;:3},{&amp;quot;key&amp;quot;:&amp;quot;/coreos.com&amp;quot;,&amp;quot;dir&amp;quot;:true,&amp;quot;modifiedIndex&amp;quot;:16,&amp;quot;createdIndex&amp;quot;:16}]}}
                     nil
controller-manager   Healthy   ok        nil
Cluster validation succeeded
Done, listing cluster services:

Kubernetes master is running at http://127.0.0.1:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;See Part 2 for setting up the server for starting up on reboot.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>django and docker</title>
      <link>http://blog.guthnur.net/django-and-docker/</link>
      <pubDate>Sun, 22 Nov 2015 19:03:28 -0800</pubDate>
      
      <guid>http://blog.guthnur.net/django-and-docker/</guid>
      <description>

&lt;p&gt;This guide shows you how to setup a Django Application and development environment using Docker and Postgres.&lt;/p&gt;

&lt;h4 id=&#34;1-install-the-docker-toolbox&#34;&gt;1. Install the Docker Toolbox&lt;/h4&gt;

&lt;p&gt;The first step is to install the &lt;a href=&#34;https://docs.docker.com/installation/&#34;&gt;docker toolbox&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On this page, find your platform and run the installation. On a Mac, you&amp;rsquo;ll be installing Docker, Docker Compose, and Docker Machine. Docker Machine will use a Linux Virtual Machine to actually run Docker.&lt;/p&gt;

&lt;h4 id=&#34;2-docker-quickstart-terminal-mac&#34;&gt;2. Docker Quickstart Terminal (Mac)&lt;/h4&gt;

&lt;p&gt;If you&amp;rsquo;re using a Mac, you will want to start working with Docker by opening the Docker Quickstart Terminal. This will ensure that your environment is setup properly. Since Docker is actually running in a VM.&lt;/p&gt;

&lt;h4 id=&#34;3-get-familiar-with-docker&#34;&gt;3. Get familiar with Docker&lt;/h4&gt;

&lt;p&gt;For our Django app we are going to build a custom Django image. There is a lot to learn about Docker images in the future, so you should definitely read up on them when your ready.&lt;/p&gt;

&lt;p&gt;For this demo, you will want to create a directory to store all your files. I&amp;rsquo;ve created a directory called ~/build/django-docker. You can do this with:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p ~/build/django-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and go to this directory&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd ~/build/django-docker
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now create a file in this directory called Dockerfile&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vim Dockerfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the following to the Dockerfile:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM python:2.7
ENV PYTHONUNBUFFERED 1
RUN mkdir /code
WORKDIR /code
ADD requirements.txt /code/
RUN pip install -r requirements.txt
ADD . /code/
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;4-create-your-requirements-txt-file&#34;&gt;4. Create your requirements.txt file&lt;/h4&gt;

&lt;p&gt;The requirements.txt file contains the python modules necessary to run your application. In this case when need to install Django and psycopg2 (postgres + python). The Dockerfile we created in the previous setup will install these required modules.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;touch requirements.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And open this file to edit. Add the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;django
psycopg2
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;5-create-your-docker-compose-yml-file&#34;&gt;5. Create your docker-compose.yml file&lt;/h4&gt;

&lt;pre&gt;&lt;code&gt;touch docker-compose.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And open this file to edit. Add the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;db:
  image: postgres
web:
  build: .
  command: python manage.py runserver 0.0.0.0:8000
  volumes:
    - .:/code
  ports:
    - &amp;quot;8000:8000&amp;quot;
  links:
    - db
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&#34;6-create-your-django-project&#34;&gt;6. Create your Django project&lt;/h4&gt;

&lt;p&gt;You&amp;rsquo;ll need to use the docker-compose run command to start your django project. Of course, if you&amp;rsquo;ve already got a project started this setup can be skipped. You might still find this helpful to read through.&lt;/p&gt;

&lt;p&gt;In your docker-compose.yml file, we have specified the command we want to run as python manage.py runserver 0.0.0.0:8000. This is the command that will be run when we bring up our web container using docker-compose up. But before we can get to that point, we actually need a django project. To do this we will need to run a command against our web service using docker-compose run&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-compose run web django-admin.py startproject exampleproject .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may already be familiar with djangos startproject command, but when using Docker we will have to run this command inside the of our container. Once you run this command you can run ls -l and take a look at the file that were created in your current directory. You will see that your project was created and manage.py was added, but both are owned by root. This is because the container runs as root. You&amp;rsquo;ll want to change the ownership by running sudo chown -R $USER:$USER .&lt;/p&gt;

&lt;h4 id=&#34;7-configure-django-to-connect-to-the-database&#34;&gt;7. Configure Django to connect to the Database&lt;/h4&gt;

&lt;p&gt;Django&amp;rsquo;s database settings are in the settings.py file located in your primary app directory &lt;code&gt;examplepoject/settings.py&lt;/code&gt; Go ahead and open this file to edit.&lt;/p&gt;

&lt;p&gt;Search for DATABASES and ensure the configuration looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;DATABASE = {
  &#39;default&#39;: {
    &#39;ENGINE&#39;: &#39;django.db.backends.postgresql_psycopg2&#39;,
    &#39;NAME&#39;: &#39;postgres&#39;,
    &#39;USER&#39;: &#39;postgres&#39;,
    &#39;HOST&#39;: &#39;db&#39;,
    &#39;PORT&#39;: 5342
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Notice the hostname. If you look back at your docker-compose file, this is the name of the database service we&amp;rsquo;are creating. When we link the database container to the web container, we are able to access the container using the name of the service as the hostname.&lt;/p&gt;

&lt;h4 id=&#34;8-run-docker-compose-up&#34;&gt;8. Run docker-compose up&lt;/h4&gt;

&lt;p&gt;At this point we&amp;rsquo;re ready to take a look at our empty application. Run docker-compose up to start the django server.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-compose up
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;re on a Mac you&amp;rsquo;ll need to grab the ip of your Docker virtual machine by running:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-machine ip default
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;otherwise you can use localhost. In my case the ip is 192.168.99.100. So open a browser and visit &lt;a href=&#34;http://192.168.99.100:8000&#34;&gt;http://192.168.99.100:8000&lt;/a&gt;. Again if you look at the docker-compose file under the ports directive we are forwarding port 8000 from our container to port 8000 on our machine running docker.&lt;/p&gt;

&lt;h4 id=&#34;9-add-data-persistence&#34;&gt;9. Add data persistence&lt;/h4&gt;

&lt;p&gt;For development, you may want to add a persistent data container. Whenever you start a new container from an image, you are starting completely fresh. That means when you start a new postgres container, it doesn&amp;rsquo;t start with any data. You&amp;rsquo;ll have to run migrations again, and you will have lost any data you my have added to some other container. This may seem odd at first, but in the end it&amp;rsquo;s essential to the portability of containers. So in theory, we can create a data only container that will be mounted onto our postgres container.&lt;/p&gt;

&lt;p&gt;To do this, lets first create an image called pg_data. To do this we will need to create another Dockerfile. I normally create a directory called docker to manage my docker-related files. So this is what I&amp;rsquo;l do, but you can put the file wherever you&amp;rsquo;d like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p docker/dockerfiles/pg_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then edit the file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd docker/dockerfiles/pg_data
vim Dockerfile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add the following to the Dockerfile&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM busybox
VOLUME /var/lib/postgresql
CMD [&amp;quot;true&amp;quot;]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now save the file and go ahead and create the image:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker build -t pg_data .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Finally, navigate back to the root directory of this app, and edit the docker-compose.yml file. Add the following to mount our data only container.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;web:
  build: .
  command: python manage.py runserver 0.0.0.0:8000
  volumes:
    - .:/code
  ports:
    - &amp;quot;8000:8000&amp;quot;
  links:
    - db
db:
  image: postgres
  volumes_from:
    - pg_data
pg_data:
  image: pg_data
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll see we added a pg_data service and we&amp;rsquo;re mounting a data volume from pg_Data onto our db container. So now, as you develop and create data, as long as you mount this data only container to your future postgres containers you will have persistent data.&lt;/p&gt;

&lt;h4 id=&#34;10-running-tests&#34;&gt;10. Running tests&lt;/h4&gt;

&lt;p&gt;Running test is fairly straight forward. You can run a basic test using the docker-compose run command.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-compose run web python manage.py test
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But what if you want to automate the test? I was recently inspired to automate a test in my deployment script. So when running my deployment script, I would first spin up a docker container, run tests, and if the tests pass I can continue with the deployment. Otherwise, we stop and fix the issues.&lt;/p&gt;

&lt;p&gt;I create a test script:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
python manage.py test --noinput 2&amp;gt; /var/log/test.log 1&amp;gt; /dev/null

if [ $? -ne 0];then
  cat /var/log/test.log
  exit 1
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And then in my deployment script I added the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker-compose run --rm web ./bin/test.sh

if [ $? -ne 0 ];then
  echo &amp;quot;Tests did not pass! Fix them!&amp;quot;
  exit 1
fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &amp;ndash;rm flag removes the containers immediately after they stop.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Containers are not VMs</title>
      <link>http://blog.guthnur.net/containers-are-not-vms/</link>
      <pubDate>Sat, 18 Jul 2015 23:53:49 -0400</pubDate>
      
      <guid>http://blog.guthnur.net/containers-are-not-vms/</guid>
      <description>

&lt;p&gt;So lately I have been digging deeper and deeper into containers and there
usefulness. It seems that a lot of people blur the line between VM&amp;rsquo;s and
Containers. I think its important that we define the two here before we get to
invested in this article.&lt;/p&gt;

&lt;h5 id=&#34;vm-concepts&#34;&gt;VM Concepts&lt;/h5&gt;

&lt;p&gt;A VM per its name is a Virtual Machine, so this by default is Read/Write
enabled. Where your changes aren&amp;rsquo;t lost in reboots or host shutdowns. This is
great for things where your not using source control. And your machine isn&amp;rsquo;t
defined as Code!.&lt;/p&gt;

&lt;h5 id=&#34;lxc-containers&#34;&gt;LXC Containers&lt;/h5&gt;

&lt;p&gt;Now lxc containers are read/write or even read only containers. This have the
same issue as VM&amp;rsquo;s and that is they arent defined by code. They have act like
VM&amp;rsquo;s or act in the ideaism&amp;rsquo;s of Docker containers if you wish. This dont have
a hypervisor, cgroups can be a nightmare to get working depending on our OS
they my or not work.&lt;/p&gt;

&lt;h5 id=&#34;docker-containers&#34;&gt;Docker Containers&lt;/h5&gt;

&lt;p&gt;Now Docker containers are defined as Code, but they are read only. So you can&amp;rsquo;t
use them as a vm and except your changes to stick unless you define it in the
code that makes the container. These typically only host one service, or one
task. Such as a Web Server or a Database server. Their cgroups work out of the
box.&lt;/p&gt;

&lt;h5 id=&#34;container-concepts&#34;&gt;Container Concepts&lt;/h5&gt;

&lt;p&gt;So the ideaism behind containers is this you have a service say nginx and your
going to host your website. 99% of the time your not going to need to increase
the number of nginx servers but the services that run php, ruby,etc. So you
would a nginx docker container, a php-fpm container set and haproxy. Your nginx
container would point to the HAProxy container which in turn points to the
php-fpm containers that you would scale based on demand. This is called one
service or one task containerism.&lt;/p&gt;

&lt;p&gt;So now that we have that cleared up. At work we are making the move to
containers and really considering docker over lxc. We have a lot of old school
thought of we need to make a quick fix just ssh to the containers and make the
fix. This is all good as long as you make sure the change is done at the
container code level before its forgotten. So that the next build of the
container has those changes. Moving to Infrastructure as Code from
a Infrastructure with 500 physical machines is a hard concept for a lot of
people to get behind. It requires a whole different train of thought. You have
to forget the idea of oh I can just ssh to there and do what I need. This is
called system drift. Your system configuration management tool should be the
only thing making changes. This is what I call Infrastructure as Pseudo Code.&lt;/p&gt;

&lt;p&gt;So how do we avoid drift in machines, or never having a machine at its
originally state ? Well this is where containers come in to play. You have the
host machine that is completely bare nothing more than what it needs to do its
job configured. Only services that should be installed are sshd, iscsi for
storage and nfs also for storage. Along with your container service of choice.
You only connect to the host when you want to get a container setup and
running.&lt;/p&gt;

&lt;p&gt;So you probably wondering what about a development environment. Local
development is how it should be done. Your developers should be able to have
access to resources to stand up a small production environment on their
machines. This eliminates the need to have a VM in production for developers to
work from. Since our Infrastructure is code now, the developer can just do
something simple like using a docker compose file to stand up a web, cache,
search and db all in a few minutes.&lt;/p&gt;

&lt;p&gt;This is a new way of thinking and it takes a lot of time to get people to see
that this style is how the industry is moving and to understand it. This is
a change that will not happen over night. If you grab some of your senior
developers and show them the process and how they can finally have a replica
environment that our clients have, this is priceless to them. Now they can
truly debug and aren&amp;rsquo;t tied to a vpn connection or a local network connection.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>